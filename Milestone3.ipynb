{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ADA Project : Milestone 2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Cracking the Hollywood interview\n",
    "\n",
    "On average, the production of a major box office movie costs $65 million, without counting the marketing and distribution fees. Unlike house construction, which usually ends up exactly like the pre-sketched plan, making a movie is unpredictable and anticipating the audienceâ€™s opinion is nearly impossible despite the effort and money spent.\n",
    "Thus, producing it the right way is a crucial job that requires long-time studies and decision-making about the relevant parts that define the movie. This includes the storyline, the script, the actors, the budget, and many more.\n",
    "This motivates our goal of studying the successful as well as failed films in terms of public ratings collected through the IMDB databases as well as the box office revenue. We mainly analyze the different metrics that define a movie in order to come up with a set of criteria that, if present, will more likely make it successful."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1) Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import nltk\n",
    "import gzip\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from __future__ import annotations\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import patsy.builtins as pat\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2) Utility functions\n",
    "\n",
    "Some basic functions that may be helpful later."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def freebase_to_wiki_id(freebase_id: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Query Wikidata ID from Freebase ID\n",
    "    :param freebase_id: str\n",
    "    :return: corresponding wiki ID\n",
    "    \"\"\"\n",
    "    url = f\"https://www.wikidata.org/w/index.php?search={freebase_id}&title=Special%3ASearch&go=Go\"\n",
    "    res = BeautifulSoup(requests.get(url).text, \"html.parser\")\\\n",
    "        .find(\"div\", {\"class\": \"mw-search-result-heading\"})\n",
    "    if res is not None:\n",
    "        return res.find(\"a\")[\"href\"].split(\"/\")[-1]\n",
    "    return None\n",
    "\n",
    "path_corenlp = '../data/corenlp_plot_summaries/'\n",
    "\n",
    "def parse_summaries(file: str) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Parse XML npl processed summary\n",
    "    :param file: name of\n",
    "    :return: BeautifulSoup object\n",
    "    \"\"\"\n",
    "    with gzip.open(path_corenlp + file, 'rb') as f:\n",
    "        data = f.read()\n",
    "    return BeautifulSoup(data, features=\"xml\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3) Movie metadata analysis and processing\n",
    "\n",
    "We start by analyzing and cleaning the movies' metadata file ``movie.metadata.tsv``."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.1) Getting more revenue data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "metadata_df = pd.read_csv('data/movie.metadata.tsv', sep=\"\\t\")\n",
    "metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get some insights\n",
    "metadata_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since the revenue is an important feature in our study, let's see if we have enough data:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df['Movie box office revenue'].isna().sum() / metadata_df.shape[0] * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Around 90% of the movie revenues are absent. We used the ``boxoffice.csv`` dataset to enrich our dataset:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def extract_release_year(df):\n",
    "    \"\"\"\n",
    "    Exctracts the release year from the release date as a new column\n",
    "    :param df: the dataframe\n",
    "    :return: The extended dataframe\n",
    "    \"\"\"\n",
    "    df['Movie release year'] = df['Movie release date'].astype(str).str[:4]\n",
    "    df = df[df[\"Movie release year\"].str.contains(\"nan\") == False].copy()\n",
    "    df['Movie release year'] = df['Movie release year'].apply(lambda x: int(x))\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add a release year column for convenience\n",
    "metadata_df = extract_release_year(metadata_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the revenue dataset\n",
    "revenue = pd.read_csv('data/boxoffice.csv', sep=',')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fill some of the absent revenue fields from the boxoffice dataset\n",
    "metadata_df.set_index(['Movie name','Movie release year'])\n",
    "revenue.set_index(['title','year'])\n",
    "metadata_df['Movie box office revenue'].fillna(revenue['lifetime_gross'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df['Movie box office revenue'].isna().sum() / metadata_df.shape[0] * 100"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We gained around 20% of relevant data!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2) Additional data on movies' ratings\n",
    "\n",
    "We add information about movie's ratings thanks to the IMDb dataset.\n",
    "The IMDb dataset is described at <https://www.imdb.com/interfaces/> , which is a version granted by IMDb for academic purposes. We have asked and being authorised by the IMDb team to have access to it.\n",
    "\n",
    "The following gets IMDb movies' ratings from ``imdb_title_ratings.tsv``."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMDb ratings dataset:**\n",
    "\n",
    "- **tconst**: unique identifier of the movie\n",
    "- **averageRating**: average of user ratings\n",
    "- **numVotes**: number of ratings submitted for the movie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMDb ratings dataset\n",
    "imdb_ratings_df = pd.read_csv('data/imdb_title_ratings.tsv', sep='\t')\n",
    "imdb_ratings_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMDb videos' metadata dataset:**\n",
    "\n",
    "- **tconst**: unique identifier of title\n",
    "- **titleType**: type of the title (movie, TV series, etc.)\n",
    "- **primaryTitle**: title the film is known for\n",
    "    - **originalTitle**: original title (in original language) of film\n",
    "- **isAdult**: indicator about adult category\n",
    "- **startYear**: release year\n",
    "- **endYear**: end year (for series)\n",
    "- **runtimeMinutes**: duration of the title in minutes\n",
    "- **genres**: up to three genres associated to the title"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMDb videos' metadata dataset\n",
    "imdb_names_df = pd.read_csv('data/imdb_title_basics.tsv', sep='\t', low_memory=False)\n",
    "# Only keep audiovisual products labeled as movies\n",
    "imdb_names_df = imdb_names_df[imdb_names_df['titleType'] == 'movie']\n",
    "imdb_names_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join between IMDb movies' rating and metadata\n",
    "imdb_ratings_meta = imdb_ratings_df.merge(imdb_names_df, on='tconst', how='inner')\n",
    "imdb_ratings_meta.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We merge the original CMU dataset with the IMDb dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Join between CMU and IMDb datasets\n",
    "ratings_merge = metadata_df.merge(imdb_ratings_meta, left_on='Movie name', right_on = 'primaryTitle', how='inner')\n",
    "ratings_merge.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the joined datasets by keeping entries that are consistent in their release dates. In other words, release year has to be similar in both datasets, otherwise we consider an entry may contain erroneous or noisy data and we discard it."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Format movie release date\n",
    "ratings_merge = extract_release_year(ratings_merge)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter pairs (obtained by join) by almost same release year (max. difference of two)\n",
    "ratings_merge = ratings_merge[ratings_merge['startYear'] != '\\\\N']\n",
    "ratings_merge['releaseDiff'] = (ratings_merge['Movie release year'].astype(int) - ratings_merge['startYear'].astype(int)).abs()\n",
    "ratings_merge = ratings_merge[ratings_merge['releaseDiff'] <= 2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We also clean the joined datasets with respect to the movie's runtime."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filter pairs (obtained by join) by almost same runtime (max. difference of 10% with respect of the CMU duration)\n",
    "# First, we handle the problem of non-existing runtime information for some entries\n",
    "ratings_merge = ratings_merge[ratings_merge['runtimeMinutes'] != '\\\\N']\n",
    "ratings_merge['Movie runtime'].fillna(-1, inplace=True)\n",
    "\n",
    "ratings_merge['runtimeDiff'] = (ratings_merge['Movie runtime'].astype(int) - ratings_merge['runtimeMinutes'].astype(int)).abs()\n",
    "ratings_merge = ratings_merge[ratings_merge['runtimeDiff'] <= ratings_merge['Movie runtime']/10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings_merge.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We clean the 'ratings_merge' dataframe (which is the result of joining IMDb's ratings and CMU datasets).\n",
    "We remove duplicated or non-relevant columns and we order columns in more logical order (with adjacent columns containing similar information)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Remove unnecessary columns\n",
    "ratings_merge = ratings_merge.drop('releaseDiff', axis=1)\n",
    "ratings_merge = ratings_merge.drop('runtimeDiff', axis=1)\n",
    "ratings_merge = ratings_merge.drop('endYear', axis=1)\n",
    "ratings_merge = ratings_merge.drop('Movie name', axis=1)\n",
    "ratings_merge = ratings_merge.drop('Movie release date', axis=1)\n",
    "ratings_merge = ratings_merge.drop('runtimeMinutes', axis=1)\n",
    "ratings_merge = ratings_merge.drop('titleType', axis=1)\n",
    "\n",
    "# Reorder columns\n",
    "ratings_merge = ratings_merge[['Wikipedia movie ID', 'Freebase movie ID', 'tconst', 'primaryTitle', 'originalTitle', 'Movie box office revenue', 'averageRating', 'numVotes', 'Movie runtime', 'Movie languages', 'Movie countries (Freebase ID:name tuples)', 'Movie genres (Freebase ID:name tuples)', 'genres', 'isAdult', 'startYear']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings_merge.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that 'ratings_merge' represent the an extended 'metadata_df' dataframe (with rating information and more consistent entries), we refer this 'ratings_merge' as the new 'metadata_df'."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Assignment for extended dataframe\n",
    "metadata_df = ratings_merge\n",
    "\n",
    "# Useful labels for revenue and ratings\n",
    "box_office_rev = 'Movie box office revenue'\n",
    "box_office_rat = 'averageRating'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we have a dataframe complete with non null revenues and ratings for sufficiently many movies, ready for use:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df = metadata_df[metadata_df[box_office_rev].notnull() & metadata_df[box_office_rat].notnull()].copy()\n",
    "metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "IMDb genres processing:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df[\"Movie genres: values\"] = metadata_df[\"genres\"].apply(lambda x: x.split(\",\"))\n",
    "metadata_df.drop(\"genres\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3) Initial revenue and rating distributions\n",
    "\n",
    "In this section, we examine basic facts and distributions of our data, starting with the global distribution of the revenue."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlabel('Box office revenue', fontsize = 15)\n",
    "plt.ylabel('Number of movies ', fontsize=15)\n",
    "plt.title('Box office revenue of all the movies', fontsize=15)\n",
    "sns.histplot(data=metadata_df[box_office_rev], log_scale=True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Revenue seems to be slightly heavy tailed. Indeed:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df[box_office_rev].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The mean is 3.528042e+07, whilst the median is 5.656388e+06, an order of magnitude smaller.\n",
    "Let's examine the ratings' distribution:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 5))\n",
    "plt.xlabel('Movie rating', fontsize = 15)\n",
    "plt.ylabel('Number of movies ', fontsize=15)\n",
    "plt.title('IMDb rating of all the movies', fontsize=15)\n",
    "sns.histplot(data=metadata_df[box_office_rat])\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This distribution seems closer to normal, with a few peaks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df[box_office_rat].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Median and mean are close. Is the distribution really approximately normal?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats.normaltest(metadata_df[box_office_rat])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "p-value is basically 0, we can confidently reject the hypothesis that the data comes from a normal distribution."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's have a quick look at the influence of the year when a movie was released on both rating and revenue."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sums_year = metadata_df.groupby('startYear')[[box_office_rev]].sum()[1:]\n",
    "means_year = metadata_df.sort_values(by=\"startYear\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_by_year(df: pd.Series | pd.DataFrame, prefix: str, metric: str, col: str, log_yscale=True):\n",
    "    \"\"\"\n",
    "    Plot a year grouped column\n",
    "    :param df: said column\n",
    "    :param prefix: str\n",
    "    :param metric: what is being plotted\n",
    "    :param log_yscale: bool\n",
    "    :param col: where to find data in df\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    fig.set_size_inches(16, 8)\n",
    "    ax.set_xlabel('Release year', fontsize=18)\n",
    "    ax.set_ylabel(f'{prefix} {metric}', fontsize=16)\n",
    "    ax.set_title(f'{prefix} {metric} by year', fontsize=16)\n",
    "    ax.set_label(\"Floats\")\n",
    "    if prefix == \"Mean\":\n",
    "        l = sns.lineplot(data=df, x=\"startYear\", y=col, legend=\"brief\")\n",
    "    else:\n",
    "        l = sns.lineplot(data=df, legend=\"brief\")\n",
    "    if log_yscale:\n",
    "        l.set_yscale(\"log\")\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_by_year(means_year[80:-10], \"Mean\", \"box office revenue\", \"Movie box office revenue\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "An upwards trend of mean expected revenue is to be expected, especially since more money gets thrown into the film industry:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_by_year(sums_year, \"Total\", \"box office revenue\", \"Movie box office revenue\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now let us do the same for rating:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_by_year(means_year[80:-10], \"Mean\", \"rating\", \"averageRating\", log_yscale=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interestingly, between the 1960 and the 1985, IMDb average ratings have dropped, before plateauing. We will have to see if this is tied to the number of ratings submitted. Now, what about the dependence between rating and revenue?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats.spearmanr(metadata_df[box_office_rev], metadata_df[box_office_rat])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The p-value being this small rejects that rating and revenue are uncorrelated. However, that correlation seems to be small. Later, it might be interesting to have a look at this correlation *per year* rather than globally."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lastly, we also plot the runtime just to check whether there is anything interesting to be seen:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=metadata_df[\"Movie runtime\"])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df[\"Movie runtime\"].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_by_year(means_year, \"Mean\", \"movie runtime\", \"Movie runtime\", log_yscale=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Runtime has increased during the first half of the 20th century. It might be worth studying how runtime affects rating and revenue."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4) Effect of genre, country and language on rating and revenue\n",
    "\n",
    "We move on to the meat and potatoes of metadata analysis: how genre, country and language affect rating and revenue. Some preprocessing first to add indicator variables for each genre, country and language."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_first_and_last_chars(input_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Remove the first and last char of a string.\n",
    "    :param input_str\n",
    "    :return: processed string\n",
    "    \"\"\"\n",
    "    if len(input_str) < 2:\n",
    "        return \"\"\n",
    "    return input_str[1:-1]\n",
    "\n",
    "def separate_id_from_data(paired_string: str, remove_brackets=True) -> (str, str):\n",
    "    \"\"\"\n",
    "    Separates \"{\"FreebaseID\": \"some string\"}\" strings into (\"FreebaseID\", \"some string\") tuples.\n",
    "    :param paired_string: input paired string\n",
    "    :param remove_brackets: if True, remove the leading and trailing curly brackets\n",
    "    :return: said tuple of strings\n",
    "    \"\"\"\n",
    "    if remove_brackets:\n",
    "        paired_string = remove_first_and_last_chars(paired_string)\n",
    "\n",
    "    ls = paired_string.split(\":\")\n",
    "    for i, s in enumerate(ls):\n",
    "        ls[i] = remove_first_and_last_chars(s.strip())\n",
    "\n",
    "    if len(ls) < 2:\n",
    "        return None, None\n",
    "    return ls[0], ls[1]\n",
    "\n",
    "def separate_ids_from_list_data(list_paired_string: str) -> (list, list):\n",
    "    \"\"\"\n",
    "    Separates \"{\"FreebaseID1\": \"some string 1\", \"FreebaseID2\": \"some string 2\", etc.}\" strings\n",
    "    into two lists: ([\"FreebaseID1\", \"FreebaseID2\", ...], [\"some string 1\", \"some string 2\", ...]).\n",
    "    :param list_paired_string: input list of pairs as string\n",
    "    :return: said tuple of lists\n",
    "    \"\"\"\n",
    "    list_paired_string = remove_first_and_last_chars(list_paired_string)\n",
    "    split_pairs = list_paired_string.split(\",\")\n",
    "    tupled_pairs = [separate_id_from_data(pair, remove_brackets=False) for pair in split_pairs]\n",
    "    return [p[0] for p in tupled_pairs], [p[1] for p in tupled_pairs]\n",
    "\n",
    "def col_to_col_values(column_name: str) -> str:\n",
    "    \"\"\"\n",
    "    String formatting for value column names\n",
    "    :param column_name: original column name\n",
    "    :return: said formatted string\n",
    "    \"\"\"\n",
    "    return f\"{column_name}: values\"\n",
    "\n",
    "def append_processed_columns(df: pd.DataFrame, column_name: str):\n",
    "    \"\"\"\n",
    "    Separate Freebase IDs from values\n",
    "    :param df: data, modified in place\n",
    "    :param column_name: name of column where to separate {Freebase ID: value} pairs\n",
    "    \"\"\"\n",
    "    vals = df[column_name].apply(separate_ids_from_list_data).values.copy()\n",
    "    df[f\"{column_name}: Freebase IDs\"] = [vals[i][0] for i in range(len(vals))]\n",
    "    df[col_to_col_values(column_name)] = [vals[i][1] for i in range(len(vals))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "metadata_df.rename(columns={\"Movie countries (Freebase ID:name tuples)\": \"Movie countries\",\n",
    "                    \"Movie genres (Freebase ID:name tuples)\": \"Movie genres\"}, inplace=True)\n",
    "\n",
    "cols_to_process = [\"Movie countries\", \"Movie genres\", \"Movie languages\"]\n",
    "for col in cols_to_process:\n",
    "    append_processed_columns(metadata_df, col)\n",
    "\n",
    "metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def distinct_values(column_name: str, raw_name: bool=False) -> set:\n",
    "    \"\"\"\n",
    "    Get all values from a column\n",
    "    :param column_name: said column\n",
    "    :return: set of values\n",
    "    \"\"\"\n",
    "    col_name = column_name if raw_name else col_to_col_values(column_name)\n",
    "    return set.union(*metadata_df[col_name].apply(set).values)\n",
    "\n",
    "def name_appended_column(prefix: str, val: str) -> str:\n",
    "    \"\"\"\n",
    "    Format column name\n",
    "    :param prefix: str\n",
    "    :param val: str\n",
    "    :return: str\n",
    "    \"\"\"\n",
    "    return f\"{prefix}: {val}\"\n",
    "\n",
    "def append_indicator_columns(df: pd.DataFrame, all_values: set, column_name: str, prefix: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Add columns to the right of a dataframe indicating whether a particular value is present or not\n",
    "    in some initial column listing values of the same family\n",
    "    :param df: data (not modified)\n",
    "    :param all_values: all possible values\n",
    "    :param column_name: column to inspect\n",
    "    :param prefix: str\n",
    "    :return: Dataframe with added columns\n",
    "    \"\"\"\n",
    "    cols = [df[col_to_col_values(column_name)]\n",
    "            .apply(lambda x: 1 if val in x else 0)\n",
    "            .rename(name_appended_column(prefix, val))\n",
    "            for val in all_values]\n",
    "    cols.insert(0, df)\n",
    "    return pd.concat(cols, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Store all genres and countries encountered:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "all_countries = distinct_values(\"Movie countries\")\n",
    "all_genres = distinct_values(\"Movie genres\")\n",
    "all_languages = distinct_values(\"Movie languages\")\n",
    "\n",
    "genre_prefix = \"genre\"\n",
    "langs_prefix = \"lang\"\n",
    "cntry_prefix = \"country\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then, let's check the resulting dataframe:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for ls in [(all_countries, \"Movie countries\", cntry_prefix),\n",
    "            (all_genres, \"Movie genres\", genre_prefix),\n",
    "            (all_languages, \"Movie languages\", langs_prefix)]:\n",
    "    metadata_df = append_indicator_columns(metadata_df, *ls)\n",
    "\n",
    "metadata_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For pragmatic purposes such as visualization, we will only treat, in the following, either the $n$ most frequent genres, or genres that are sufficiently frequent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from functools import cmp_to_key\n",
    "\n",
    "def retrieve_n_most_frequent(df: pd.DataFrame, n: int, all_vals: list[str], prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Retrieve the n most frequent genres, languages or countries, sorted in descending order\n",
    "    of frequency\n",
    "    :param df: data\n",
    "    :param n: integer, max number of values to retrieve\n",
    "    :param all_vals: all possible values\n",
    "    :param prefix: str\n",
    "    :return: said list\n",
    "    \"\"\"\n",
    "    def comparator(val1, val2):\n",
    "        mean_val1 = df[name_appended_column(prefix, val1)].mean()\n",
    "        mean_val2 = df[name_appended_column(prefix, val2)].mean()\n",
    "        return mean_val1 - mean_val2\n",
    "    return sorted(all_vals, key=cmp_to_key(comparator), reverse=True)[:n]\n",
    "\n",
    "def retrieve_frequent(df: pd.DataFrame, all_vals: list, prefix: str, freq_threshold=0.05) -> list:\n",
    "    \"\"\"\n",
    "    Filter the values with a sufficiently high frequency\n",
    "    :param df: data\n",
    "    :param all_vals: all possible values\n",
    "    :param prefix: str\n",
    "    :param freq_threshold: float\n",
    "    :return: list of sufficiently frequent values\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        filter(\n",
    "            lambda val: df[name_appended_column(prefix, val)].mean() > freq_threshold,\n",
    "            all_vals\n",
    "        )\n",
    "    )\n",
    "\n",
    "def map_to_col_names(data_names: list, prefix: str) -> list:\n",
    "    \"\"\"\n",
    "    Convert data values into column names\n",
    "    :param data_names: list of  data values\n",
    "    :param prefix: str\n",
    "    :return: list of formatted column names\n",
    "    \"\"\"\n",
    "    f = lambda x: name_appended_column(prefix, x)\n",
    "    return list(map(f, data_names))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "most_freq_genres = retrieve_n_most_frequent(metadata_df, 35, all_genres, genre_prefix)\n",
    "suff_freq_genres = retrieve_frequent(metadata_df, all_genres, genre_prefix, freq_threshold=0.03)\n",
    "\n",
    "most_freq_langs = retrieve_n_most_frequent(metadata_df, 35, all_languages, langs_prefix)\n",
    "suff_freq_langs = retrieve_frequent(metadata_df, all_languages, langs_prefix, freq_threshold=0.03)\n",
    "\n",
    "most_freq_cntry = retrieve_n_most_frequent(metadata_df, 35, all_countries, cntry_prefix)\n",
    "suff_freq_cntry = retrieve_frequent(metadata_df, all_countries, cntry_prefix, freq_threshold=0.03)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our next step will be to examine genres, countries and languages that have are correlated to rating and revenue, and try to understand which ones optimize either metric."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def find_correlated_metadata(df: pd.DataFrame, freq_data: list, success_metric: str, prefix: str, sig_level=0.05) -> list:\n",
    "    \"\"\"\n",
    "    Among a list of sufficiently frequent data taken from the metadata dataframe,\n",
    "    find the values such that they are correlated to a movie's success metric with\n",
    "    a p-value less than sig-level.\n",
    "    :param df: input dataframe\n",
    "    :param freq_data: column names to search in\n",
    "    :param success_metric: str, name of column in df\n",
    "    :param prefix: str\n",
    "    :param sig_level: significance level, defaults to 5%\n",
    "    :return: described list\n",
    "    \"\"\"\n",
    "    correlated_data = []\n",
    "\n",
    "    for value in freq_data:\n",
    "        res = stats.spearmanr(df[success_metric], df[name_appended_column(prefix, value)])\n",
    "        if res.pvalue < sig_level:\n",
    "            correlated_data.append(value)\n",
    "    return correlated_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "correlated_genres_to_revenue = find_correlated_metadata(metadata_df, suff_freq_genres, box_office_rev, genre_prefix)\n",
    "correlated_langs_to_revenue = find_correlated_metadata(metadata_df, most_freq_langs, box_office_rev, langs_prefix)\n",
    "correlated_cntry_to_revenue = find_correlated_metadata(metadata_df, most_freq_cntry, box_office_rev, cntry_prefix)\n",
    "\n",
    "correlated_genres_to_rating = find_correlated_metadata(metadata_df, suff_freq_genres, box_office_rat, genre_prefix)\n",
    "correlated_langs_to_rating = find_correlated_metadata(metadata_df, most_freq_langs, box_office_rat, langs_prefix)\n",
    "correlated_cntry_to_rating = find_correlated_metadata(metadata_df, most_freq_cntry, box_office_rat, cntry_prefix)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_metadata_frequency_against_metric(df: pd.DataFrame, prefix: str,  titled_data: list, success_metric: str, title: str, log_scale=True):\n",
    "    \"\"\"\n",
    "    Generating a grid of histograms\n",
    "    :param df: data\n",
    "    :param prefix: str\n",
    "    :param titled_data: titles of data, to be converted to column names\n",
    "    :param success_metric: measured column name\n",
    "    :param title: str, figure title\n",
    "    :param log_scale: determines the scale of the axes\n",
    "    \"\"\"\n",
    "\n",
    "    # Making the data fit into a square grid...\n",
    "    squares = np.arange(8) ** 2\n",
    "    shifted_squares = squares - len(titled_data)\n",
    "    smallest_big_enough_square = squares[np.argmax(shifted_squares > 0) - 1]\n",
    "\n",
    "    tested_data = map_to_col_names(titled_data, prefix)[:smallest_big_enough_square]\n",
    "    size = int(np.sqrt(smallest_big_enough_square))\n",
    "\n",
    "    fig, ax = plt.subplots(size, size, figsize=(11, 11), sharex = True)\n",
    "    for i in range(smallest_big_enough_square):\n",
    "        sbplt = ax[i%size, math.floor(i/size)]\n",
    "        sns.histplot(ax=sbplt, data=df[df[tested_data[i]] == 1][success_metric], log_scale=log_scale)\n",
    "        sbplt.set_title(titled_data[i])\n",
    "\n",
    "    fig.suptitle(title, fontsize=18)\n",
    "    fig.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, genre_prefix, correlated_genres_to_revenue, box_office_rev, title=\"Plotting revenue against genres\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, genre_prefix, correlated_genres_to_rating, box_office_rat, title=\"Plotting rating against genres\", log_scale=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, cntry_prefix, correlated_cntry_to_revenue, box_office_rev, title=\"Plotting revenue against countries\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, cntry_prefix, correlated_cntry_to_rating, box_office_rat, title=\"Plotting rating against countries\", log_scale=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, langs_prefix, correlated_langs_to_revenue, box_office_rev, title=\"Plotting revenue against languages\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_metadata_frequency_against_metric(metadata_df, langs_prefix, correlated_langs_to_rating, box_office_rat, title=\"Plotting rating against languages\", log_scale=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Interesting differences can already be seen. For example, the 3 initial peaks in the rating histogram have become 5 peaks in English drama movies coming from the US. Later, it will be interesting to check the correlation of rating and revenue on a *per year* basis too."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4) Actors Metadata analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we focus on analysing the characters metadata in ``charcater.metadata.tsv``"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "names_char = ['Wikipedia movie ID',\n",
    "              'Freebase movie ID',\n",
    "              'Movie release date',\n",
    "              'Character name',\n",
    "              'Actor date of birth',\n",
    "              'Actor gender',\n",
    "              'Actor height',\n",
    "              'Actor ethnicity',\n",
    "              'Actor name',\n",
    "              'Actor age at movie release',\n",
    "              'Freebase character/actor map ID',\n",
    "              'Freebase character ID',\n",
    "              'Freebase actor ID']\n",
    "\n",
    "characters_metadata_df = pd.read_csv('data/character.metadata.tsv', sep='\\t', names = names_char)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df['Actor gender'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Age at movie release:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df['Actor age at movie release'].describe()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We noticed that the attribute 'Age at movie release' contains negative values"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df[characters_metadata_df['Actor age at movie release'] < 0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "characters_metadata_df['Actor age at movie release'] = characters_metadata_df['Actor age at movie release'].apply(abs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.histplot(data=characters_metadata_df, x=\"Actor age at movie release\")\n",
    "plt.xlim(0, 100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Titles known for:\n",
    "Since our study focuses on analysing features that contribute to a movie's success, it would be relevant to study actors performance. This can be done by having information on when each actor performed the best.\n",
    "For this task, we used the  ``imdb_name_basics.tsv`` dataset, which contains the 4 topmost titles an actor is known for."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMDb actors dataset:**\n",
    "\n",
    "- **nconst**: unique identifier of person\n",
    "- **primaryName**: name the person is known for\n",
    "- **birthYear**: birth year\n",
    "- **deathYear**: death year\n",
    "- **primaryProfession**: top 3 professions of the person\n",
    "- **knownForTitles**: movies the person is known for\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the IMDb actors dataset\n",
    "imdb_actors_df = pd.read_csv('data/imdb_name_basics.tsv', sep='\t')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we only have the id of the known for titles, we fetch the title names from the `imdb_names_df`"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we separate each of the titles ids an actor is known for in a single column, then we fetch the name corresponding to each id."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_title_by_index(index):\n",
    "    \"\"\"\n",
    "    Returns the title id at a given index\n",
    "    :param index: The id index in the list of ids\n",
    "    :return: The id\n",
    "    \"\"\"\n",
    "    def f(array):\n",
    "        list_ids = array.split(\",\")\n",
    "        corresponding_id = \"\"\n",
    "        if index < len(list_ids):\n",
    "            corresponding_id = list_ids[index]\n",
    "        return corresponding_id\n",
    "\n",
    "    return f\n",
    "\n",
    "imdb_actors_df['knownForTitles1'] = imdb_actors_df['knownForTitles'].apply(get_title_by_index(0))\n",
    "imdb_actors_df['knownForTitles2'] = imdb_actors_df['knownForTitles'].apply(get_title_by_index(1))\n",
    "imdb_actors_df['knownForTitles3'] = imdb_actors_df['knownForTitles'].apply(get_title_by_index(2))\n",
    "imdb_actors_df['knownForTitles4'] = imdb_actors_df['knownForTitles'].apply(get_title_by_index(3))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# IMDb movies' metadata dataset\n",
    "imdb_names_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Keep only relevant columns\n",
    "imdb_names_reduced_df = imdb_names_df[[\"tconst\", \"primaryTitle\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def assign_known_title(index, actors_df, names_df):\n",
    "    \"\"\"\n",
    "    Fetch the known titles names\n",
    "    :param index: Index of the title in the list\n",
    "    :param actors_df: actors dataframe\n",
    "    :param names_df: movie names dataframe\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    actors_df = actors_df.merge(names_df, left_on=\"knownForTitles{}\".format(index), right_on = 'tconst', how='inner')\n",
    "    actors_df[\"knownForTitles{}\".format(index)] = actors_df['primaryTitle']\n",
    "    actors_df = actors_df.drop('tconst', axis=1)\n",
    "    actors_df = actors_df.drop('primaryTitle', axis=1)\n",
    "    return actors_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df = assign_known_title(1, imdb_actors_df, imdb_names_reduced_df)\n",
    "imdb_actors_df = assign_known_title(2, imdb_actors_df, imdb_names_reduced_df)\n",
    "imdb_actors_df = assign_known_title(3, imdb_actors_df, imdb_names_reduced_df)\n",
    "imdb_actors_df = assign_known_title(4, imdb_actors_df, imdb_names_reduced_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df['knownForTitlesNames'] = imdb_actors_df[imdb_actors_df.columns[6:]].apply(\n",
    "    lambda x: ','.join(x),\n",
    "    axis=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df = imdb_actors_df.drop('knownForTitles1', axis = 1).drop('knownForTitles2', axis = 1).drop('knownForTitles3', axis = 1).drop('knownForTitles4', axis = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "imdb_actors_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As an example, let's see what are the titles Leonardo Dicaprio is known for"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = imdb_actors_df[imdb_actors_df['primaryName'] == \"Leonardo DiCaprio\"]['knownForTitlesNames']\n",
    "print(result)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No surprise !"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We add the IMDb's casting dataset. It contains the people that have participated in castings for movies and their role in them.\n",
    "We are not directly using this dataframe in Milestone 2 as it will be joined and applied to data obtained by methods in Milestone 3 (refer to Method 2 at Readme for further information)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**IMDb castings dataset:**\n",
    "\n",
    "- **tconst**: unique identifier of title\n",
    "- **ordering**: index identify a row within a specific title\n",
    "- **nconst**: unique identifier of person\n",
    "- **category**: category of job in casting\n",
    "- **job**: job title\n",
    "- **characters**: name of character if it was an interpretation role"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/imdb_title_principals.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/8l/38mw0fgd64x6gy7hbrcdfb1r0000gn/T/ipykernel_786/1208702136.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# IMDb castings dataset\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mimdb_casting_df\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_csv\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'data/imdb_title_principals.tsv'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msep\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'   '\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlow_memory\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mimdb_names_df\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m                     \u001B[0mstacklevel\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstacklevel\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    310\u001B[0m                 )\n\u001B[0;32m--> 311\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    312\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    313\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36mread_csv\u001B[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001B[0m\n\u001B[1;32m    584\u001B[0m     \u001B[0mkwds\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mupdate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mkwds_defaults\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    585\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 586\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0m_read\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_read\u001B[0;34m(filepath_or_buffer, kwds)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    481\u001B[0m     \u001B[0;31m# Create the parser.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 482\u001B[0;31m     \u001B[0mparser\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mTextFileReader\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilepath_or_buffer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    483\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    484\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mchunksize\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0miterator\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, f, engine, **kwds)\u001B[0m\n\u001B[1;32m    809\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"has_index_names\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    810\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 811\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_engine\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_make_engine\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    812\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    813\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py\u001B[0m in \u001B[0;36m_make_engine\u001B[0;34m(self, engine)\u001B[0m\n\u001B[1;32m   1038\u001B[0m             )\n\u001B[1;32m   1039\u001B[0m         \u001B[0;31m# error: Too many arguments for \"ParserBase\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1040\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mmapping\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mengine\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moptions\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# type: ignore[call-arg]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1041\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1042\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_failover_to_python\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, src, **kwds)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     50\u001B[0m         \u001B[0;31m# open handles\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 51\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_open_handles\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwds\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     52\u001B[0m         \u001B[0;32massert\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mhandles\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/base_parser.py\u001B[0m in \u001B[0;36m_open_handles\u001B[0;34m(self, src, kwds)\u001B[0m\n\u001B[1;32m    220\u001B[0m         \u001B[0mLet\u001B[0m \u001B[0mthe\u001B[0m \u001B[0mreaders\u001B[0m \u001B[0mopen\u001B[0m \u001B[0mIOHandles\u001B[0m \u001B[0mafter\u001B[0m \u001B[0mthey\u001B[0m \u001B[0mare\u001B[0m \u001B[0mdone\u001B[0m \u001B[0;32mwith\u001B[0m \u001B[0mtheir\u001B[0m \u001B[0mpotential\u001B[0m \u001B[0mraises\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    221\u001B[0m         \"\"\"\n\u001B[0;32m--> 222\u001B[0;31m         self.handles = get_handle(\n\u001B[0m\u001B[1;32m    223\u001B[0m             \u001B[0msrc\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    224\u001B[0m             \u001B[0;34m\"r\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py\u001B[0m in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    700\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mencoding\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0;34m\"b\"\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    701\u001B[0m             \u001B[0;31m# Encoding\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 702\u001B[0;31m             handle = open(\n\u001B[0m\u001B[1;32m    703\u001B[0m                 \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    704\u001B[0m                 \u001B[0mioargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'data/imdb_title_principals.tsv'"
     ]
    }
   ],
   "source": [
    "# IMDb castings dataset\n",
    "imdb_casting_df = pd.read_csv('data/imdb_title_principals.tsv', sep='\t', low_memory=False)\n",
    "imdb_names_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5) tvtropes clusters analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we use the clusters in ``tvtropes.clusters.txt`` to see what type of characters an actor tends to play."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the data into a dataframe\n",
    "with open('data/tvtropes.clusters.txt') as f:\n",
    "    d = []\n",
    "    for row in f.readlines():\n",
    "        t, json_data = row.split('\\t')\n",
    "        data = json.loads(json_data)\n",
    "        data['type'] = t\n",
    "        d.append(data)\n",
    "\n",
    "tv_tropes_df = pd.DataFrame(d)\n",
    "tv_tropes_df.set_index(\"id\", inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tv_tropes_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split the type in tokens\n",
    "tv_tropes_df['type'] = tv_tropes_df['type'].apply(lambda x: ' '.join(x.split('_')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tv_tropes_df['type'].head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In order to have better insight on the data and also for further analysis on what type of characters are played by each actor, we decide to conduct sentiment analysis on those type. For this task we use the nltk library."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the model\n",
    "nltk.download('vader_lexicon')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialization and example\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "sia.polarity_scores('adventurer archeologist')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_sentiment(sentence):\n",
    "    \"\"\"\n",
    "    Compute the sentiment value of a sentence\n",
    "    :param sentence: The sentence\n",
    "    :return: -1 for negative, 0 for neutral and 1 for positive\n",
    "    \"\"\"\n",
    "    score = sia.polarity_scores(sentence)\n",
    "    # If the score is more positive with a less significant neutral value and vice versa\n",
    "    if score['pos'] > score['neg'] and score['neu'] < 0.5: return 1\n",
    "    elif score['pos'] < score['neg'] and score['neu'] < 0.5: return -1\n",
    "    else: return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a column sentiment containing the sentiment value of the character type\n",
    "tv_tropes_df['sentiment'] = tv_tropes_df['type'].apply(get_sentiment).to_frame()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tv_tropes_df.head(10)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "tv_tropes_df['sentiment'].value_counts().plot(kind='bar', xlabel= 'sentiment', ylabel = 'count', title = 'count of each sentiment')\n",
    "neg_patch = mpatches.Patch(label='-1 = negative')\n",
    "pos_patch = mpatches.Patch(label='1 = positive')\n",
    "neu_patch = mpatches.Patch(label='0 = neutral')\n",
    "blue_patch = mpatches.Patch(color='blue', label='Positive')\n",
    "\n",
    "plt.legend(handles=[neg_patch, pos_patch, neu_patch])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 6) Plot analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we do some processing on the plot summaries for further use in Milestone 3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the plot summaries\n",
    "plots_df = pd.read_csv('data/plot_summaries.txt', sep='\t')\n",
    "plots_df = plots_df[plots_df[\"wikipedia_id\"].isin(metadata_df[\"Wikipedia movie ID\"])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plots_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(plots_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "subpath = \"data/corenlp_plot_summaries/\"\n",
    "starting_positions = {\"VB\", \"NN\", \"NP\", \"PP\", \"RB\"}\n",
    "import os\n",
    "\n",
    "def get_important_lemmas(wiki_id: int) -> list[str]:\n",
    "    \"\"\"\n",
    "    Retrieve important lemmas from\n",
    "    :param wiki_id: wikipedia movie id\n",
    "    :return: list of lemmas of important words\n",
    "    \"\"\"\n",
    "\n",
    "    def is_important(token):\n",
    "        tok = token.find(\"POS\").text\n",
    "        for pos in starting_positions:\n",
    "            if tok.startswith(pos):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def to_lemma(token):\n",
    "        return token.find(\"lemma\").text\n",
    "\n",
    "    zip_name = str(wiki_id) + \".xml.gz\"\n",
    "\n",
    "    if not os.path.isfile(subpath + zip_name):\n",
    "        print(f\"Missing file with id {wiki_id}\")\n",
    "        return []\n",
    "\n",
    "    with gzip.open(subpath + zip_name, 'r') as file:\n",
    "        xmltree = ET.ElementTree(ET.fromstring(file.read())).getroot()\n",
    "\n",
    "    return list(map(to_lemma, filter(is_important, xmltree.iter(\"token\"))))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%time\n",
    "pickle_path = \"data/pickled_data/plots_pickle.pkl\"\n",
    "if os.path.isfile(pickle_path):\n",
    "    plots_df = pd.read_pickle(pickle_path)\n",
    "else:\n",
    "    plots_df['important_lemmas'] = plots_df['wikipedia_id'].apply(get_important_lemmas)\n",
    "    plots_df.to_pickle(pickle_path)\n",
    "plots_df.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "len(plots_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lengths = plots_df[\"important_lemmas\"].apply(len)\n",
    "print(f\"{len(plots_df[lengths == 0])} movie(s) has/have no core nlp plot summary.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer=lambda x: x)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
